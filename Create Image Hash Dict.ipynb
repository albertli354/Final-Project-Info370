{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np, PIL, pandas as pd, json, re, pickle, os\n",
    "from hashlib import sha1 as hash_fn\n",
    "\n",
    "from time import time, localtime, asctime, ctime\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import Markdown\n",
    "from stat import S_ISREG, ST_CTIME, ST_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a94a8fe5ccb19ba61c4c0873d391e987982fbbd3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_fn(\"test\".encode(\"UTF-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 20\n",
    "end = 40\n",
    "\n",
    "step_size=20\n",
    "\n",
    "chunk_file_suffix = [i for i in range(start, end, step_size)]\n",
    "\n",
    "base_path = \"../\"\n",
    "\n",
    "data_dir = \"data/intermediate/\"\n",
    "chunking = \"chunking/\"\n",
    "hashes = \"hashes/\"\n",
    "\n",
    "base_mnist_path = base_path+\"mnist/\"\n",
    "by_field_dir = \"by_field/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hash_pickle(img_hash_dict, file_count):\n",
    "    \"\"\"\n",
    "        Writes a pickle of the passed obj, with filename specifying the hash function, the timestamp and the passed file count\n",
    "        dict: to serialize\n",
    "        file_count: shows how many files are processed at a glance (make resuming easier)\n",
    "        \n",
    "        returns the filename the pickle is written to\n",
    "    \"\"\"\n",
    "    #generate a unique filename component\n",
    "    timestamp = asctime(localtime(time())).replace(\" \", \"_\").replace(\":\", \".\")\n",
    "\n",
    "    #create the filename, including the hash algo\n",
    "    pickle_filename = \"by_field_{}_{}_{}.pickle\".format(hash_fn.__name__, timestamp, str(file_count))\n",
    "    pickle_path = data_dir+hashes+pickle_filename\n",
    "    \n",
    "    #open the file\n",
    "    img_hashes = open(pickle_path, \"wb\")\n",
    "    \n",
    "    #dump the hash dictionary\n",
    "    pickle.dump(img_hash_dict, img_hashes)\n",
    "\n",
    "    #close the file\n",
    "    img_hashes.close()\n",
    "\n",
    "    return pickle_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hash_pickle(filename):\n",
    "    \"\"\"\n",
    "        Read a pickled img hash dictionary, requires filename\n",
    "        returns image hash based dictionary\n",
    "    \"\"\"\n",
    "    file = open(filename, \"rb\")\n",
    "    img_hash_dict = pickle.loads(file.read())\n",
    "    file.close()\n",
    "    return img_hash_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '4A', '00000']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(\"^([0-9A-Fa-f]{2})_\", \"4A_00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1551842784: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_19.26.24_2019_10000.pickle',\n",
       " 1551843561: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_19.39.20_2019_20000.pickle',\n",
       " 1551843619: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_19.40.19_2019_30000.pickle'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get newest filename\n",
    "path = data_dir+hashes\n",
    "\n",
    "hash_pickles = os.listdir(path)\n",
    "\n",
    "pickle_stats = {os.stat(path+pickle)[ST_CTIME]:path+pickle for pickle in hash_pickles}\n",
    "display(pickle_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Loading Img Hash Pickle for 30000 files processed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pickle_key_list = [key for key in pickle_stats.keys()]\n",
    "pickle_key_list.sort(reverse=True)\n",
    "\n",
    "pickle_path = None\n",
    "reloaded = None\n",
    "\n",
    "start_file = 0\n",
    "\n",
    "if len(pickle_key_list)>0:\n",
    "    pickle_path = pickle_stats[pickle_key_list[0]]\n",
    "    pickle_name_split = re.split(r\"_([0-9]+)\", pickle_path)\n",
    "    start_file = int(pickle_name_split[-2])\n",
    "    \n",
    "display(Markdown(\"Loading Img Hash Pickle for {} processed files\".format(start_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pickle_path is not None:\n",
    "    reloaded = read_hash_pickle(pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd63768e2e4a409595e027368bd1627f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_0/lower/63/63_00132.png\n",
       " ## New File: ../mnist/by_field/hsf_0/upper/43/43_00119.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_0/lower/66/66_00125.png\n",
       " ## New File: ../mnist/by_field/hsf_0/upper/46/46_00118.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_0/lower/7a/7A_00128.png\n",
       " ## New File: ../mnist/by_field/hsf_0/upper/5a/5A_00123.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_1/const/65/65_01702.png\n",
       " ## New File: ../mnist/by_field/hsf_1/const/6e/6E_00678.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_1/const/6e/6E_00925.png\n",
       " ## New File: ../mnist/by_field/hsf_1/const/71/71_00076.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_1/const/56/56_00450.png\n",
       " ## New File: ../mnist/by_field/hsf_1/const/72/72_02464.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_2/digit/31/31_05710.png\n",
       " ## New File: ../mnist/by_field/hsf_2/digit/31/31_05713.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_2/lower/6a/6A_00292.png\n",
       " ## New File: ../mnist/by_field/hsf_2/upper/4a/4A_00320.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_4/digit/30/30_00000.png\n",
       " ## New File: ../mnist/by_field/hsf_4/digit/30/30_00001.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_4/lower/72/72_00000.png\n",
       " ## New File: ../mnist/by_field/hsf_4/lower/72/72_00001.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Collision!\n",
       " ## Existing file: ../mnist/by_field/hsf_4/upper/44/44_00000.png\n",
       " ## New File: ../mnist/by_field/hsf_4/upper/44/44_00001.png"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create dict by hash values\n",
    "hash_img_dict = {}\n",
    "\n",
    "if reloaded:\n",
    "    hash_img_dict = reloaded\n",
    "    start\n",
    "    #reloaded = {}\n",
    "\n",
    "fp = FloatProgress(min=0, max=100)\n",
    "\n",
    "files_per_pickle_write = 10000\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "display(fp)\n",
    "for (root, dirs, files) in os.walk(base_mnist_path+by_field_dir):\n",
    "    clean_path = root.replace(\"\\\\\", \"/\")\n",
    "    \n",
    "    if len(files) > 0:\n",
    "        \n",
    "        for filename in files:\n",
    "            fp.value = (fp.value+1)%100\n",
    "            file_path = clean_path+\"/\"+filename\n",
    "            filename_pieces = re.split(\"^([0-9A-Za-z]{2})_\", filename)\n",
    "            \n",
    "            if len(filename_pieces) < 2:\n",
    "                continue\n",
    "            \n",
    "            if start_file > file_count:\n",
    "                file_count += 1\n",
    "                continue\n",
    "            \n",
    "            this_hasher = hash_fn()\n",
    "            with open(file_path, \"rb\") as im:\n",
    "                data = im.read()\n",
    "                if data is None:\n",
    "                    display(Markdown(\"# Error reading file\"))\n",
    "                    \n",
    "                this_hasher.update(data)\n",
    "                \n",
    "            #explicitly close the file for better resource management\n",
    "            im.close()\n",
    "            \n",
    "            digest = this_hasher.hexdigest()\n",
    "            \n",
    "            if digest in hash_img_dict.keys():\n",
    "                existing_file_with_hash = hash_img_dict[digest][\"file_path\"]\n",
    "                markdown_msg = \"# Collision!\\n ## Existing file: {}\\n ## New File: {}\".format(existing_file_with_hash, file_path)\n",
    "                display(Markdown(markdown_msg))\n",
    "            \n",
    "            #display(filename_pieces, digest)\n",
    "            \n",
    "                \n",
    "            code = filename_pieces[1]\n",
    "            #display(code)\n",
    "            hash_img_dict[digest] = {\"file_path\": file_path, \"char_label\": code}\n",
    "            \n",
    "            file_count += 1\n",
    "            \n",
    "            if file_count % files_per_pickle_write == 0:\n",
    "                write_hash_pickle(hash_img_dict, file_count)\n",
    "                \n",
    "pickle_path = write_hash_pickle(hash_img_dict, \"all\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = read_hash_pickle(pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded == hash_img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '../mnist/by_field/hsf_0/const/57/57_00000.png',\n",
       " 'char_label': '57'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This is an image of the character with code 0x57 which is \"W\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'file_path': '../mnist/by_field/hsf_0/const/57/57_00000.png',\n",
       " 'char_label': '57'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This is an image of the character with code 0x57 which is \"W\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "by_writer_dir = \"by_write/\"\n",
    "\n",
    "this_hash = hash_fn()\n",
    "\n",
    "with open(base_mnist_path+by_writer_dir+\"hsf_0/f0000_14/c0000_14/c0000_14_00000.png\", \"rb\") as test:    \n",
    "    data = test.read()\n",
    "    this_hash.update(data)\n",
    "    \n",
    "digest = this_hash.hexdigest()\n",
    "    \n",
    "if digest in hash_img_dict.keys():\n",
    "    entry = hash_img_dict[digest]\n",
    "    display(entry)\n",
    "    code = int(entry[\"char_label\"], 16)\n",
    "    display(Markdown(\"This is an image of the character with code 0x{:x} which is \\\"{}\\\"\".format(code, chr(code))))\n",
    "else:\n",
    "    display(\"Test image not found!\")\n",
    "    \n",
    "if digest in reloaded.keys():\n",
    "    entry = reloaded[digest]\n",
    "    display(entry)\n",
    "    code = int(entry[\"char_label\"], 16)\n",
    "    display(Markdown(\"This is an image of the character with code 0x{:x} which is \\\"{}\\\"\".format(code, chr(code))))\n",
    "else:\n",
    "    display(\"Test image not found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
