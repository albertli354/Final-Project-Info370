{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np, PIL, pandas as pd, json, re, pickle, os\n",
    "from hashlib import sha1 as hash_fn\n",
    "\n",
    "from time import time, localtime, asctime\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a94a8fe5ccb19ba61c4c0873d391e987982fbbd3'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_fn(\"test\".encode(\"UTF-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 20\n",
    "end = 40\n",
    "\n",
    "step_size=20\n",
    "\n",
    "chunk_file_suffix = [i for i in range(start, end, step_size)]\n",
    "\n",
    "data_dir = \"data/intermediate/\"\n",
    "chunking = \"chunking/\"\n",
    "hashes = \"hashes/\"\n",
    "\n",
    "base_mnist_path = \"../mnist/\"\n",
    "by_field_dir = \"by_field/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hash_pickle(img_hash_dict, file_count):\n",
    "    \"\"\"\n",
    "        Writes a pickle of the passed obj, with filename specifying the hash function, the timestamp and the passed file count\n",
    "        dict: to serialize\n",
    "        file_count: shows how many files are processed at a glance (make resuming easier)\n",
    "        \n",
    "        returns the filename the pickle is written to\n",
    "    \"\"\"\n",
    "    #generate a unique filename component\n",
    "    timestamp = asctime(localtime(time())).replace(\" \", \"_\").replace(\":\", \".\")\n",
    "\n",
    "    #create the filename, including the hash algo\n",
    "    pickle_filename = \"by_field_{}_{}_{}.pickle\".format(hash_fn.__name__, timestamp, str(file_count))\n",
    "    pickle_path = data_dir+hashes+pickle_filename\n",
    "    \n",
    "    #open the file\n",
    "    img_hashes = open(pickle_path, \"wb\")\n",
    "    \n",
    "    #dump the hash dictionary\n",
    "    pickle.dump(img_hash_dict, img_hashes)\n",
    "\n",
    "    #close the file\n",
    "    img_hashes.close()\n",
    "\n",
    "    return pickle_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hash_pickle(filename):\n",
    "    \"\"\"\n",
    "        Read a pickled img hash dictionary, requires filename\n",
    "        returns image hash based dictionary\n",
    "    \"\"\"\n",
    "    file = open(filename, \"rb\")\n",
    "    img_hash_dict = pickle.loads(file.read())\n",
    "    file.close()\n",
    "    return img_hash_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608a2bcf8943438f84df786260d71bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create dict by hash values\n",
    "hash_img_dict = {}\n",
    "\n",
    "fp = FloatProgress(min=0, max=100)\n",
    "\n",
    "files_per_pickle_write = 1000\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "display(fp)\n",
    "for (root, dirs, files) in os.walk(base_mnist_path+by_field_dir):\n",
    "    clean_path = root.replace(\"\\\\\", \"/\")\n",
    "    \n",
    "    if len(files) > 0:\n",
    "        \n",
    "        for filename in files:\n",
    "            fp.value = (fp.value+1)%100\n",
    "            file_path = clean_path+\"/\"+filename\n",
    "            filename_pieces = re.split(\"([0-9]{2})_\", filename)\n",
    "            this_hasher = hash_fn()\n",
    "            with open(file_path, \"rb\") as im:\n",
    "                data = im.read()\n",
    "                if data is None:\n",
    "                    display(Markdown(\"# Error reading file\"))\n",
    "                    \n",
    "                this_hasher.update(data)\n",
    "                \n",
    "            #explicitly close the file for better resource management\n",
    "            im.close()\n",
    "            \n",
    "            digest = this_hasher.hexdigest()\n",
    "            \n",
    "            if digest in hash_img_dict.keys():\n",
    "                existing_file_with_hash = hash_img_dict[digest][\"file_path\"]\n",
    "                markdown_msg = \"# Collision!\\n ## Existing file: {} New File: {}\".format(existing_file_with_hash, file_path)\n",
    "                display(Markdown(markdown_msg))\n",
    "            \n",
    "            #display(filename_pieces, digest)\n",
    "            \n",
    "            if len(filename_pieces) < 2:\n",
    "                continue\n",
    "                \n",
    "            code = filename_pieces[1]\n",
    "            #display(code)\n",
    "            hash_img_dict[digest] = {\"file_path\": file_path, \"char_label\": code}\n",
    "            \n",
    "            file_count += 1\n",
    "            \n",
    "            if file_count % files_per_pickle_write == 0:\n",
    "                write_hash_pickle(hash_img_dict, file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_hash_pickle(hash_img_dict, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = read_hash_pickle(\"data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_18.19.19_2019_0.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded == hash_img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_writer_dir = \"by_write/\"\n",
    "\n",
    "this_hash = hash_fn()\n",
    "\n",
    "with open(base_mnist_path+by_writer_dir+\"hsf_0/f0000_14/c0000_14/c0000_14_00000.png\", \"rb\") as test:    \n",
    "    data = test.read()\n",
    "    this_hash.update(data)\n",
    "    \n",
    "digest = this.hexdigest()\n",
    "    \n",
    "if digest in hash_img_dict.keys():\n",
    "    hash_img_dict[digest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
