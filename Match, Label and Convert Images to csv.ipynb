{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np, PIL, pandas as pd, json, re, pickle, os\n",
    "from hashlib import sha1 as hash_fn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time, localtime, asctime, ctime\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import Markdown, DisplayHandle\n",
    "\n",
    "from stat import S_ISREG, ST_CTIME, ST_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 20\n",
    "end = 40\n",
    "\n",
    "step_size=20\n",
    "\n",
    "chunk_file_suffix = [i for i in range(start, end, step_size)]\n",
    "\n",
    "base_path = \"../\"\n",
    "\n",
    "data_dir = \"data/intermediate/\"\n",
    "chunking = \"chunking/\"\n",
    "hashes = \"hashes/\"\n",
    "csvs = \"csvs/\"\n",
    "\n",
    "base_mnist_path = base_path+\"mnist/\"\n",
    "by_field_dir = \"by_field/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hash_pickle(filename):\n",
    "    \"\"\"\n",
    "        Read a pickled img hash dictionary, requires filename\n",
    "        returns image hash based dictionary\n",
    "    \"\"\"\n",
    "    file = open(filename, \"rb\")\n",
    "    img_hash_dict = pickle.loads(file.read())\n",
    "    file.close()\n",
    "    return img_hash_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get newest filename\n",
    "def get_newest_pickle():\n",
    "    path = data_dir+hashes\n",
    "\n",
    "    hash_pickles = os.listdir(path)\n",
    "\n",
    "    pickle_stats = {os.stat(path+pickle)[ST_CTIME]:path+pickle for pickle in hash_pickles}\n",
    "    display(pickle_stats)\n",
    "    pickle_key_list = [key for key in pickle_stats.keys()]\n",
    "\n",
    "    pickle_key_list.sort(reverse=True)\n",
    "\n",
    "    pickle_path = None\n",
    "    reloaded = None\n",
    "\n",
    "    start_file = 0\n",
    "\n",
    "    if len(pickle_key_list)>0:\n",
    "        pickle_path = pickle_stats[pickle_key_list[0]]\n",
    "        pickle_name_split = re.split(r\"_([0-9]+)\", pickle_path)\n",
    "        start_file = int(pickle_name_split[-2])\n",
    "\n",
    "    display(Markdown(\"Loading Img Hash Pickle for {} processed files\".format(start_file)))\n",
    "    \n",
    "    if pickle_path is not None:\n",
    "        reloaded = read_hash_pickle(pickle_path)\n",
    "        \n",
    "    return reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1551848557: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_21.01.19_2019_790000.pickle.zip',\n",
       " 1551848627: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_21.03.47_2019_810000.pickle',\n",
       " 1551849413: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_21.16.53_2019_all.pickle',\n",
       " 1551850193: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_21.16.53_2019_all.pickle.zip',\n",
       " 1551852012: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_22.00.12_2019_all.pickle',\n",
       " 1551852031: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_22.00.31_2019_all.pickle',\n",
       " 1551852345: 'data/intermediate/hashes/by_field_openssl_sha1_Tue_Mar__5_22.05.45_2019_all.pickle'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Loading Img Hash Pickle for 2019 processed files"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_hash_dict = get_newest_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(filename):\n",
    "    \"\"\"\n",
    "        Calc hash of file given a filename\n",
    "        filename: file to hash\n",
    "        returns hexdigest hash\n",
    "    \"\"\"\n",
    "    \n",
    "    from pathlib import Path\n",
    "    \n",
    "    if not Path(filename):\n",
    "        raise ValueError(\"{} does not exist\".format(filename))\n",
    "    \n",
    "    this_hasher = hash_fn()\n",
    "            \n",
    "    with open(filename, \"rb\") as img:\n",
    "        \n",
    "        this_hasher.update(img.read())\n",
    "\n",
    "    img.close()\n",
    "\n",
    "    return this_hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'file_path': '../mnist/by_field/hsf_0/digit/30/30_00000.png',\n",
       " 'char_label': '30'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'file_path': '../mnist/by_field/hsf_0/digit/30/30_00000.png',\n",
       " 'char_label': '30'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "814244"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_file_hash = get_hash(\"../mnist/by_field/hsf_0/digit/30/30_00000.png\")\n",
    "match_file_hash = get_hash(\"../mnist/by_write/hsf_0/f0000_14/d0000_14/d0000_14_00000.png\")\n",
    "\n",
    "display(orig_file_hash == match_file_hash)\n",
    "\n",
    "\n",
    "entry = img_hash_dict[orig_file_hash]\n",
    "display(entry)\n",
    "display(img_hash_dict[match_file_hash])\n",
    "\n",
    "display(Markdown(\"{}\".format(chr(int(entry[\"char_label\"], 16)))))\n",
    "\n",
    "display(len(img_hash_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '../mnist/by_field/hsf_0/const/57/57_00000.png',\n",
       " 'char_label': '57'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This is an image of the character with code 0x57 which is \"W\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "by_writer_dir = \"by_write/\"\n",
    "\n",
    "this_hash = hash_fn()\n",
    "\n",
    "with open(base_mnist_path+by_writer_dir+\"hsf_0/f0000_14/c0000_14/c0000_14_00000.png\", \"rb\") as test:    \n",
    "    data = test.read()\n",
    "    this_hash.update(data)\n",
    "    \n",
    "digest = this_hash.hexdigest()\n",
    "    \n",
    "if digest in img_hash_dict.keys():\n",
    "    entry = img_hash_dict[digest]\n",
    "    display(entry)\n",
    "    code = int(entry[\"char_label\"], 16)\n",
    "    display(Markdown(\"This is an image of the character with code 0x{:x} which is \\\"{}\\\"\".format(code, chr(code))))\n",
    "else:\n",
    "    display(\"Test image not found!\")\n",
    "    \n",
    "# if digest in reloaded.keys():\n",
    "#     entry = reloaded[digest]\n",
    "#     display(entry)\n",
    "#     code = int(entry[\"char_label\"], 16)\n",
    "#     display(Markdown(\"This is an image of the character with code 0x{:x} which is \\\"{}\\\"\".format(code, chr(code))))\n",
    "# else:\n",
    "#     display(\"Test image not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Writer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343c421dd8214c8abb00069110755e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=4.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## File"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01de9631d7bd4e65802bbb4858286f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAB+0lEQVR4nO3c0WqDQBBG4d3S93/l7UVBRE0wRjn/6PmuSkkgzNlx05v2MUYT54f+AE9nAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmAJgBYAaAGQBmANgv/QGO671PP9f9x2vFNqD3/j/3+fTnvy+nzAbM51t01ptqbMDOiVcMUyDAR2Mt16DMI2hhfuvuGfr0mrTrOn0D1sMdY7wf4voti/sjakvSA3zp1bhzGkQHyBnTdYrdATuf4IXK5W7A5tMf+SSXyg3wEKEBwm/OE4UGuFrO06zMJfxqZG/WYnpL8uqU34DNMPM/1sIv88QNODyy9cvCp98CAxwYWdpMP5L1CLr6wAamygpwrkXOwOm3GwcoMf0WHuDw1JK/dy5EBzhL7PFvyQGecPxb2tfQK45q8vFvyRtwivDpt9sHyNfzz8i9uQEwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzAAwA8AMADMAzACwPypUXiQAcpeWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=128x128 at 0x1C258AD0860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Adding pandas row... **writer**: 0000 **char_type**:                                        c label: M"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[i for i in range((128**2))]+[\"image\", \"label\", \"writer_id\"])\n",
    "\n",
    "downsample_factor = 1 #start with no downsampling\n",
    "\n",
    "#im = Image.open(file)\n",
    "#ih = IPython.display.Image(data=im)\n",
    "#display(ih)\n",
    "\n",
    "display(Markdown(\"## Writer\"))\n",
    "\n",
    "w_fp = FloatProgress(min=0, max=4)\n",
    "display(w_fp)\n",
    "\n",
    "display(Markdown(\"## File\"))\n",
    "\n",
    "f_fp = FloatProgress(min=0, max=1)\n",
    "display(f_fp)\n",
    "\n",
    "dh_img = DisplayHandle()\n",
    "dh_img.display(Markdown(\"Begin\"))\n",
    "\n",
    "dh_stage = DisplayHandle()\n",
    "dh_stage.display(Markdown(\"Begin\"))\n",
    "\n",
    "for suffix in chunk_file_suffix:\n",
    "    dh_stage.update(Markdown(\"Loading Chunk file...\"))\n",
    "    chunk_file = data_dir+chunking+\"all_by_write_files_{}.json\".format(suffix)\n",
    "    \n",
    "    json_file = open(chunk_file)\n",
    "    \n",
    "    chunk_dict = json.loads(json_file.read())\n",
    "\n",
    "    for key, writer_char in chunk_dict.items():\n",
    "        #display(key, writer_char)\n",
    "        f_fp.value = 0\n",
    "        f_fp.max = len(writer_char[\"files\"])\n",
    "        for file in writer_char[\"files\"]:\n",
    "            writer_id = writer_char[\"writer_id\"]\n",
    "            char_type = writer_char[\"char_type\"]\n",
    "            w_fp.value = (int(writer_id)%4)+1\n",
    "            dh_stage.update(Markdown(\"Loading Image file...\\\n",
    "                                        **writer** {} **char_type**: {}\".format(writer_id, char_type)))\n",
    "            #display(file)\n",
    "            \n",
    "            \n",
    "            \n",
    "            digest = get_hash(file)\n",
    "            \n",
    "            dh_stage.update(Markdown(\"Got image file hash...\\\n",
    "                                     **writer** {} **char_type**: {}\".format(writer_id, char_type)))\n",
    "            \n",
    "            code_label = None\n",
    "            \n",
    "            entry = None\n",
    "            try:\n",
    "                entry = img_hash_dict[digest]\n",
    "                #display(entry)\n",
    "                code_label = chr(int(entry[\"char_label\"], 16))\n",
    "                dh_stage.update(Markdown(\"Found matchig image file hash...\\\n",
    "                                            **writer** {} **char_type**: {}\".format(writer_id, char_type)))\n",
    "            except KeyError as e:\n",
    "                display(Markdown(\"hash for {} not found!\".format(file)))\n",
    "                continue\n",
    "\n",
    "            dh_stage.update(Markdown(\"Processing Image file...**writer**: {} **char_type**: {} \\\n",
    "                                        label: {}\".format(writer_id, char_type, code_label)))\n",
    "            \n",
    "            im = Image.open(file)\n",
    "            dh_img.update(im)\n",
    "            \n",
    "            bit2 = im.convert(\"P\", palette=Image.ADAPTIVE, colors=256)\n",
    "            x = int(im.width/downsample_factor)  # 128\n",
    "            y = int(im.height/downsample_factor) # 128\n",
    "            bit2xy = bit2.resize((x, y), resample=PIL.Image.LANCZOS)\n",
    "            \n",
    "            data = bit2xy.getdata()\n",
    "            \n",
    "            arr = np.array(data)\n",
    "            arr = arr.reshape((x, y))\n",
    "            \n",
    "            dh_stage.update(Markdown(\"Making pandas row... **writer**: {} **char_type**: {}\\\n",
    "                                        label: {}\".format(writer_id, char_type, code_label)))\n",
    "            \n",
    "            row = {\"image\":arr, \"label\": code_label, \"writer_id\":writer_id, \"char_type\": char_type}\n",
    "            \n",
    "            row_w_pixel_cols = {index_1d: pixel for index_1d, pixel in enumerate(arr)}\n",
    "            \n",
    "            for k, v in row.items():\n",
    "                row_w_pixel_cols[k] = v\n",
    "            \n",
    "            dh_stage.update(Markdown(\"Adding pandas row... **writer**: {} **char_type**:\\\n",
    "                                        {} label: {}\".format(writer_id, char_type, code_label)))\n",
    "            df = df.append(row_w_pixel_cols, ignore_index=True)\n",
    "            #display(df.shape)\n",
    "            f_fp.value += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(data_dir+csvs+\"mnist_chunks_\"+str(start)+\"_to_\"+str(end)+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
